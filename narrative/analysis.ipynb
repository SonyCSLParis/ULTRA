{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from statsmodels.stats import multitest\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group results for all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_output_run_many(fp):\n",
    "    \"\"\" Output of zero-shot \"\"\"\n",
    "    with open(fp, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines]\n",
    "    \n",
    "    data = []\n",
    "    for l in lines:\n",
    "        if len(l.split(',')) > 1:\n",
    "            data.append(l.split(','))\n",
    "    return pd.DataFrame(data, columns=[\"dataset\", \"mr\", \"mrr\", \"hits@1\", \"hits@3\", \"hits@10\"])\n",
    "\n",
    "def add_info_dataset(row):\n",
    "    for name in [\"prop\", \"subevent\", \"role\", \"causation\"]:\n",
    "        row[name] = 1 if f\"{name.capitalize()}1\" in row[\"dataset\"] else 0\n",
    "    row[\"syntax\"] = row[\"dataset\"].split(\"Syntax\")[1]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_folder_name(folder):\n",
    "    \"\"\"Extract training parameters from folder name using regex\"\"\"\n",
    "    try:\n",
    "        bpe = int(folder.split(\"bpe_\")[1].split(\"_\")[0])\n",
    "    except:\n",
    "        bpe = 0\n",
    "    \n",
    "    ckpt = \"_\".join(folder.split(\"ckpt_\")[1].split(\"_\")[:2])\n",
    "    \n",
    "    return {\n",
    "        'checkpoint': ckpt,\n",
    "        'epochs': int(folder.split(\"epochs_\")[1].split(\"_\")[0]),\n",
    "        'batch_per_epoch': folder.split(\"bpe_\")[1].split(\"_\")[0],\n",
    "        'batch_size': int(folder.split(\"bs_\")[1].split(\"_\")[0])\n",
    "    }\n",
    "\n",
    "def read_all(folder):\n",
    "    data = []\n",
    "    modes = os.listdir(folder)\n",
    "    for m in modes:\n",
    "        print(f\"MODE: {m}\")\n",
    "        settings = os.listdir(os.path.join(folder, m))\n",
    "        for s in tqdm(settings):\n",
    "            params = get_info_folder_name(s)\n",
    "            pf = os.path.join(folder, m, s)\n",
    "            command = f\"python get_model_results.py {pf}\"\n",
    "            if not os.path.exists(os.path.join(pf, \"results.csv\")):\n",
    "                subprocess.run(command, shell=True)\n",
    "            df = pd.read_csv(os.path.join(pf, \"results.csv\"), index_col=0)\n",
    "            for k, v in params.items():\n",
    "                df[k] = v\n",
    "            df[\"mode\"] = m\n",
    "            data.append(df)\n",
    "    return pd.concat(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_all(\"experiments/inductive\")\n",
    "\n",
    "mappings = {}\n",
    "for col in [\"syntax\", \"mode\"]:\n",
    "    # Create mapping dictionary\n",
    "    categories = df[col].unique()\n",
    "    mapping = {cat: i for i, cat in enumerate(categories)}\n",
    "    # Add numeric version of the column\n",
    "    df[f\"{col}_numeric\"] = df[col].map(mapping)\n",
    "    mappings[col] = mapping\n",
    "\n",
    "for k, v in mappings.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "df[\"batch_per_epoch\"] = df[\"batch_per_epoch\"].apply(lambda x: int(x) if x.isdigit() else 0)\n",
    "df[\"ckpt_nb\"] = df[\"checkpoint\"].apply(lambda x: int(x.split(\"_\")[-1].replace(\"g\", \"\")) if x.startswith(\"ultra\") else 0)\n",
    "df['finished'] = (~((df.valid_mrr.isna()) | (df[\"valid_hits@1\"].isna()))).astype(int)\n",
    "df.to_csv(\"results/results.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of finished and unfinished experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETA_PARAMS = [\"prop\", \"subevent\", \"role\", \"causation\"]\n",
    "print(f\"# of experiments: {df.shape[0]}\")\n",
    "df_null = df[df.finished == 0]\n",
    "df_finished = df[df.finished == 1]\n",
    "print(f\"# of unfinished experiments: {df_null.shape[0]} ({round(100*df_null.shape[0]/df.shape[0])}%)\")\n",
    "df_null[ETA_PARAMS+[\"syntax\", \"epochs\", \"batch_per_epoch\", \"batch_size\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null.groupby(\"mode\").agg({\"dataset_version\": \"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null.groupby(\"causation\").agg({\"dataset_version\": \"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null.groupby([\"role\", \"syntax\"]).agg({\"dataset_version\": \"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null[(df_null.causation==1)|(df_null.role==1)].groupby([\"role\", \"syntax\"]).agg({\"dataset_version\": \"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null[df_null[\"mode\"]==\"zero-shot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlations eta_params vs. finished\")\n",
    "for col in ETA_PARAMS:\n",
    "    res_mrr = stats.spearmanr(df[col], df[\"finished\"])\n",
    "    print(f\"{col.upper()}:\\t vs. Finished: {res_mrr.statistic:.4f}, p={res_mrr.pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing syntax vs. finished\n",
    "\n",
    "# 1. Getting frequency table\n",
    "df_freq = df[(df.causation==1)|(df.role==1)].groupby([\"syntax\", \"finished\"]).agg({\"dataset_version\": \"count\"}).reset_index().pivot(index=\"syntax\", columns=\"finished\", values=\"dataset_version\").reset_index()\n",
    "df_freq.columns = [\"syntax\", \"unfinished\", \"finished\"]\n",
    "display(df_freq)\n",
    "\n",
    "# 2. Chi2-contingency\n",
    "chi2_cont_0_1 = stats.chi2_contingency(df_freq[df_freq.index!=2][[\"finished\", \"unfinished\"]])\n",
    "print(f\"Chi2 statistic 0 vs. 1: {chi2_cont_0_1.statistic:.4f}, p-value: {chi2_cont_0_1.pvalue:.4f}\")\n",
    "chi2_cont_0_2 = stats.chi2_contingency(df_freq[df_freq.index!=1][[\"finished\", \"unfinished\"]])\n",
    "print(f\"Chi2 statistic 0 vs. 2: {chi2_cont_0_2.statistic:.4f}, p-value: {chi2_cont_0_2.pvalue:.4f}\")\n",
    "chi2_cont_1_2 = stats.chi2_contingency(df_freq[df_freq.index!=0][[\"finished\", \"unfinished\"]])\n",
    "print(f\"Chi2 statistic 1 vs. 2: {chi2_cont_1_2.statistic:.4f}, p-value: {chi2_cont_1_2.pvalue:.4f}\")\n",
    "\n",
    "# 3. Holm-Bonferoni correction\n",
    "hb_correction = multitest.multipletests(\n",
    "    [chi2_cont_0_1.pvalue, chi2_cont_0_2.pvalue, chi2_cont_1_2.pvalue])\n",
    "display(hb_correction)\n",
    "\n",
    "# 4. Odds-ratio\n",
    "print(f'Odds-ratio 0 vs. 1: {stats.contingency.odds_ratio(df_freq[df_freq.index!=2][[\"finished\", \"unfinished\"]]).statistic:.4f}')\n",
    "print(f'Odds-ratio 1 vs. 2: {stats.contingency.odds_ratio(df_freq[df_freq.index!=0][[\"finished\", \"unfinished\"]]).statistic:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"syntax\", \"finished\"]).agg({\"dataset_version\": \"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_corr(df, cols):\n",
    "    for col in cols:\n",
    "        res_mrr = stats.spearmanr(df[col], df[\"valid_mrr\"])\n",
    "        res_hits1 = stats.spearmanr(df[col], df[\"valid_hits@1\"])\n",
    "        print(f\"{col.upper()}:\\t vs. MRR: {res_mrr.statistic:.4f}, p={res_mrr.pvalue:.4f} | HITS@1: {res_hits1.statistic:.4f}, p={res_hits1.pvalue:.4f}\")\n",
    "\n",
    "def print_corr_3_cat(df, col, mode_exclude):\n",
    "    res_mrr = stats.spearmanr(df[df[\"mode\"]!=mode_exclude][col], df[df[\"mode\"]!=mode_exclude][\"valid_mrr\"])\n",
    "    res_hits1 = stats.spearmanr(df[df[\"mode\"]!=mode_exclude][col], df[df[\"mode\"]!=mode_exclude][\"valid_hits@1\"])\n",
    "    print(f\"{col.upper()}:\\t vs. MRR: {res_mrr.statistic:.4f}, p={res_mrr.pvalue:.4f} | HITS@1: {res_hits1.statistic:.4f}, p={res_hits1.pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Finished with roles: {df_finished[df_finished.role==1].shape[0]}\")\n",
    "df_finished.groupby(\"mode\").agg({\"dataset_version\": \"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Spearman correlations: ALL\")\n",
    "print(\"Semantic--\")\n",
    "print_corr(df=df_finished, cols=ETA_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Syntax--\")\n",
    "print(\"Only makes sense to compare syntaxes if roles or causal relationships are present\")\n",
    "print_corr(df=df_finished[(df_finished.causation==1)|(df_finished.role==1)], cols=[\"syntax_numeric\"])\n",
    "display(df_finished[(df_finished.causation==1)|(df_finished.role==1)].groupby(\"syntax\").agg({\"valid_mrr\": [\"mean\", \"count\"], \"valid_hits@1\": \"mean\"}))\n",
    "\n",
    "print(mappings['syntax'])\n",
    "for me in df_finished[(df_finished.causation==1)|(df_finished.role==1)][\"syntax\"].unique():\n",
    "    print(f\"Correlation between {set(df_finished[(df_finished.causation==1)|(df_finished.role==1)]) - set([me])}\")\n",
    "    print_corr_3_cat(df_finished[(df_finished.causation==1)|(df_finished.role==1)], \"syntax_numeric\", me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nModel--\")\n",
    "print(\"Comparing train vs. finetune vs. zeroshot (zeroshot: 0 epochs)\")\n",
    "print(mappings['mode'])\n",
    "for me in df_finished[\"mode\"].unique():\n",
    "    print(f\"Correlation between {set(df_finished['mode'].unique()) - set([me])}\")\n",
    "    print_corr_3_cat(df_finished, \"mode_numeric\", me)\n",
    "print_corr(df=df_finished, cols=[\"epochs\", \"ckpt_nb\"])\n",
    "display(df_finished.groupby(\"mode\").agg({\"valid_mrr\": \"mean\", \"valid_hits@1\": \"mean\"}))\n",
    "print(\"Comparing model params for finetune\")\n",
    "print_corr(df=df_finished[df_finished[\"mode\"]!=\"zero-shot\"], cols=[\"batch_per_epoch\", \"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finished[(df_finished.causation==1)|(df_finished.role==1)].groupby(\"syntax\").agg({\"valid_mrr\": \"mean\", \"valid_hits@1\": \"mean\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-grained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finished[(df_finished.prop==0)&(df_finished.subevent==0)&(df_finished.role==0)&(df_finished.causation==0)].sort_values(by=[\"valid_mrr\"], ascending=False)[[\"valid_mrr\", \"valid_hits@1\", \"valid_hits@3\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common to all (also ILP, SimKGC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_static_info(df):\n",
    "    df[\"method\"] = \"ULTRA\"\n",
    "    df[\"td\"] = \"simple-triple\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP = [\n",
    "    \"ckpt_nb\", \"train_batch_per_epoch\", \"train_batch_size\", \"train_num_epoch\",\n",
    "    \"batch_per_epoch\", \"batch_size\", \"mode_numeric\"\n",
    "]\n",
    "\n",
    "METRICS = [\n",
    "    (\"test_mrr\", \"MRR\"),\n",
    "    (\"test_hits@1\", \"H@1\"),\n",
    "    (\"test_hits@3\", \"H@3\"),\n",
    "    (\"test_hits@10\", \"H@10\") \n",
    "]\n",
    "\n",
    "df_finished = df_finished.rename(columns={x: k for x, k in METRICS})\n",
    "METRICS = [x[1] for x in METRICS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETA = [\"prop\", \"subevent\", \"role\", \"causation\"]\n",
    "eta_counts = df_finished.groupby(ETA).size().reset_index(name='exp_count')\n",
    "df_finished = df_finished.merge(eta_counts, on=ETA, how='left')\n",
    "eta_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for eta, group in df_finished.groupby(ETA):\n",
    "    for hp in HP:\n",
    "        for m in METRICS:\n",
    "            res = stats.spearmanr(group[hp], group[m])\n",
    "            data.append(list(eta) + [hp, m, res.statistic, res.pvalue])\n",
    "df_corr_hp_metric = pd.DataFrame(\n",
    "    data,\n",
    "    columns=ETA + [\"hp\", \"metric\", \"corr\", \"pval\"]\n",
    ")\n",
    "df_corr_hp_metric = df_corr_hp_metric.merge(eta_counts, on=ETA, how='left')\n",
    "add_static_info(df_corr_hp_metric).to_csv(\"results/corr_hp_metric_per_eta.csv\")\n",
    "df_corr_hp_metric.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_hp_metric[df_corr_hp_metric.pval < 0.05].to_csv(\"results/corr_hp_metric_per_eta_significant.csv\")\n",
    "df_corr_hp_metric[df_corr_hp_metric.pval < 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = df_finished.groupby(ETA)[\"MRR\"].rank(method='max', ascending=False)\n",
    "df_finished[\"rank\"] = ranks\n",
    "add_static_info(df_finished[df_finished[\"rank\"]==1][ETA + HP + [\"exp_count\"]]).to_csv(\"results/best_hp_per_eta.csv\")\n",
    "df_finished[df_finished[\"rank\"]==1][ETA + HP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_syntax = df_finished[(df_finished.causation==1)|(df_finished.role==1)]\n",
    "df_syntax.to_csv(\"results/results_syntax.csv\")\n",
    "ranks_syntax = df_syntax.groupby(ETA+[\"syntax\"])[\"MRR\"].rank(method='max', ascending=False)\n",
    "df_syntax[\"rank\"] = ranks_syntax\n",
    "add_static_info(df_syntax[df_syntax[\"rank\"]==1][ETA + [\"syntax\"] + METRICS]).to_csv(\"results/best_metric_per_eta_syntax.csv\")\n",
    "df_syntax[df_syntax[\"rank\"]==1][ETA + [\"syntax\"] + METRICS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_static_info(df_finished[df_finished[\"rank\"]==1][ETA + METRICS + [\"exp_count\", \"syntax\"]]).to_csv(\"results/best_metric_per_eta.csv\")\n",
    "df_finished[df_finished[\"rank\"]==1][ETA + METRICS + HP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETA = [\"prop\", \"subevent\", \"role\", \"causation\"]\n",
    "df_paper_metric_per_eta = df_finished[df_finished[\"rank\"]==1][ETA + METRICS].copy()\n",
    "df_paper_metric_per_eta[\"sum\"] = df_paper_metric_per_eta[[\"prop\", \"subevent\", \"role\", \"causation\"]].sum(axis=1)\n",
    "\n",
    "base_mrr, base_h1, base_h3, base_h10 = df_paper_metric_per_eta[(df_paper_metric_per_eta.prop==0)&(df_paper_metric_per_eta.subevent==0)&(df_paper_metric_per_eta.role==0)&(df_paper_metric_per_eta.causation==0)][[\"MRR\", \"H@1\", \"H@3\", \"H@10\"]].values.tolist()[0]\n",
    "print(base_mrr, base_h1, base_h3, base_h10)\n",
    "\n",
    "df_paper_metric_per_eta[\"delta_MRR\"] = df_paper_metric_per_eta[\"MRR\"] - base_mrr\n",
    "df_paper_metric_per_eta[\"delta_H@1\"] = df_paper_metric_per_eta[\"H@1\"] - base_h1\n",
    "df_paper_metric_per_eta[\"delta_H@3\"] = df_paper_metric_per_eta[\"H@3\"] - base_h3\n",
    "df_paper_metric_per_eta[\"delta_H@10\"] = df_paper_metric_per_eta[\"H@10\"] - base_h10\n",
    "columns = ETA\n",
    "for col in METRICS:\n",
    "    columns.extend([col, f\"delta_{col}\"])\n",
    "df_paper_metric_per_eta.sort_values(by=[\"prop\", \"subevent\", \"role\", \"causation\"])[columns].round(2).to_csv(\"results/paper_metric_per_eta.csv\")\n",
    "df_paper_metric_per_eta.sort_values(by=[\"prop\", \"subevent\", \"role\", \"causation\"])[columns].round(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on simple settings (no causation, no roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simple = df[(df.finished == 1) & (df.role == 0) & (df.causation == 0)]\n",
    "df_simple.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_corr(df=df_simple, cols=[\"prop\", \"subevent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on zero-shot setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Group the dataframe by eta_params columns\n",
    "# 2. Calculate the maximum valid_mrr for each group\n",
    "# 3. Rank the groups based on max valid_mrr (rank 1 = highest valid_mrr)\n",
    "rank_col = \"valid_mrr\"\n",
    "mode = \"zero-shot\"\n",
    "ranks = df_finished[df_finished[\"mode\"]==mode].groupby(ETA_PARAMS+['syntax'])[rank_col].rank(method='min', ascending=False)\n",
    "df_zero_shot_ranks = df_finished[df_finished[\"mode\"]==\"zero-shot\"].copy()\n",
    "df_zero_shot_ranks['eta_rank'] = ranks\n",
    "df_zero_shot_ranks[\"ckpt_nb\"] = df_zero_shot_ranks[\"checkpoint\"].apply(lambda x: int(x.split(\"_\")[-1].replace(\"g\", \"\")))\n",
    "df_zero_shot_ranks[df_zero_shot_ranks.eta_rank==1][ETA_PARAMS + ['syntax', 'valid_mrr', 'checkpoint', 'ckpt_nb']].sort_values(by='valid_mrr', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "print(\"Spearman correlations: ZERO-SHOT\")\n",
    "curr_df = df_zero_shot_ranks[df_zero_shot_ranks.eta_rank==1]\n",
    "for col in ETA_PARAMS + ['syntax_numeric']:\n",
    "    res = stats.spearmanr(curr_df[col], curr_df[\"ckpt_nb\"])\n",
    "    print(f\"{col} vs ckpt: {res.statistic}, {res.pvalue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "for eta, group in df_zero_shot_ranks[(df_zero_shot_ranks.causation==1)|(df_zero_shot_ranks.role==1)].groupby(ETA_PARAMS):\n",
    "    print(\" | \".join([f\"{x}: {eta[i]}\" for i, x in enumerate(ETA_PARAMS)]))\n",
    "    print(group.groupby(\"syntax\").agg({\"valid_mrr\": [\"mean\", \"count\"], \"valid_hits@1\": \"mean\"}))\n",
    "    print(\"=====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing fine-tune / zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "rank_col = \"valid_mrr\"\n",
    "mode = \"fine-tune\"\n",
    "ranks = df_finished[df_finished[\"mode\"]==mode].groupby(ETA_PARAMS + [\"syntax\"])[rank_col].rank(method='min', ascending=False)\n",
    "df_fine_tune_ranks = df_finished[df_finished[\"mode\"]==mode].copy()\n",
    "df_fine_tune_ranks['eta_rank'] = ranks\n",
    "df_fine_tune_ranks[\"ckpt_nb\"] = df_fine_tune_ranks[\"checkpoint\"].apply(lambda x: int(x.split(\"_\")[-1].replace(\"g\", \"\")))\n",
    "df_fine_tune_ranks[df_fine_tune_ranks.eta_rank==1][ETA_PARAMS + [\"syntax\", 'valid_mrr', 'checkpoint', 'ckpt_nb']].sort_values(by='valid_mrr', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "common_versions = set(df_fine_tune_ranks[df_fine_tune_ranks.eta_rank==1][\"dataset_version\"]).intersection(set(df_zero_shot_ranks[\"dataset_version\"]))\n",
    "\n",
    "tc_ft = df_fine_tune_ranks[(df_fine_tune_ranks.eta_rank==1) & (df_fine_tune_ranks.dataset_version.isin(common_versions))].sort_values(by=\"dataset_version\")\n",
    "tc_zs = df_zero_shot_ranks[(df_zero_shot_ranks.eta_rank==1) & (df_zero_shot_ranks.dataset_version.isin(common_versions))].sort_values(by=\"dataset_version\")\n",
    "(tc_ft[\"valid_mrr\"] - tc_zs[\"valid_mrr\"]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "print(\"Spearman correlations: FINE-TUNE | ETA PARAMS\")\n",
    "curr_df = df_fine_tune_ranks[df_fine_tune_ranks.eta_rank==1]\n",
    "for col in ETA_PARAMS + ['syntax_numeric']:\n",
    "    res = stats.spearmanr(curr_df[col], curr_df[\"valid_mrr\"])\n",
    "    print(f\"{col} vs ckpt: {res.statistic}, {res.pvalue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "print(\"Spearman correlations: FINE-TUNE | MODEL PARAMS\")\n",
    "curr_df = df_finished[df_finished[\"mode\"]==\"fine-tune\"]\n",
    "for col in ETA_PARAMS + ['syntax_numeric'] + [\"epochs\", \"batch_per_epoch\", \"batch_size\"]:\n",
    "    res = stats.spearmanr(curr_df[col], curr_df[\"valid_mrr\"])\n",
    "    print(f\"{col} vs ckpt: {res.statistic}, {res.pvalue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "for k, v in mappings.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "fig = px.parallel_coordinates(\n",
    "    df_finished[df_finished[\"mode\"] == \"fine-tune\"], color=\"valid_mrr\",\n",
    "    dimensions=[\n",
    "        \"prop\", \"subevent\", \"role\", \"causation\", \n",
    "        \"syntax_numeric\", \"epochs\", \"batch_per_epoch\", \"batch_size\",\n",
    "        \"valid_mrr\"]\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "for k, v in mappings.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "fig = px.parallel_coordinates(\n",
    "    df, color=\"valid_mrr\",\n",
    "    dimensions=[\n",
    "        \"mode_numeric\", \"prop\", \"subevent\", \"role\", \"causation\", \n",
    "        \"syntax_numeric\", \"epochs\", \"batch_per_epoch\", \"batch_size\",\n",
    "        \"valid_mrr\"]\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ULTRA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
